import streamlit as st
import pandas as pd
import time
import hashlib
import pickle
import os
from functools import wraps
from typing import Any, Callable, Dict, Optional
import logging

class PerformanceOptimizer:
    """ì„±ëŠ¥ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, cache_dir="cache"):
        self.cache_dir = cache_dir
        self.ensure_cache_dir()
        self.setup_logging()
    
    def ensure_cache_dir(self):
        """ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def setup_logging(self):
        """ì„±ëŠ¥ ë¡œê¹… ì„¤ì •"""
        self.perf_logger = logging.getLogger('performance')
        self.perf_logger.setLevel(logging.INFO)
        
        # ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼
        perf_handler = logging.FileHandler(
            os.path.join(self.cache_dir, 'performance.log'),
            encoding='utf-8'
        )
        perf_handler.setFormatter(
            logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        )
        self.perf_logger.addHandler(perf_handler)
    
    def measure_time(self, func_name: str = None):
        """ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                
                execution_time = end_time - start_time
                name = func_name or func.__name__
                
                # ë¡œê¹…
                self.perf_logger.info(f"{name} executed in {execution_time:.4f} seconds")
                
                # ëŠë¦° í•¨ìˆ˜ ê²½ê³  (2ì´ˆ ì´ìƒ)
                if execution_time > 2.0:
                    self.perf_logger.warning(f"Slow function detected: {name} took {execution_time:.4f}s")
                
                return result
            return wrapper
        return decorator
    
    def cache_dataframe(self, cache_key: str, ttl_hours: int = 1):
        """DataFrame ìºì‹± ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ìºì‹œ íŒŒì¼ ê²½ë¡œ
                cache_file = os.path.join(self.cache_dir, f"{cache_key}.pickle")
                
                # ìºì‹œ í™•ì¸
                if os.path.exists(cache_file):
                    # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
                    file_age = time.time() - os.path.getmtime(cache_file)
                    if file_age < ttl_hours * 3600:  # TTL ì²´í¬
                        try:
                            with open(cache_file, 'rb') as f:
                                cached_data = pickle.load(f)
                            self.perf_logger.info(f"Cache hit for {cache_key}")
                            return cached_data
                        except Exception as e:
                            self.perf_logger.error(f"Cache read error for {cache_key}: {e}")
                
                # ìºì‹œ ë¯¸ìŠ¤ - í•¨ìˆ˜ ì‹¤í–‰
                result = func(*args, **kwargs)
                
                # ê²°ê³¼ ìºì‹±
                try:
                    with open(cache_file, 'wb') as f:
                        pickle.dump(result, f)
                    self.perf_logger.info(f"Cache stored for {cache_key}")
                except Exception as e:
                    self.perf_logger.error(f"Cache write error for {cache_key}: {e}")
                
                return result
            return wrapper
        return decorator
    
    def optimize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """DataFrame ë©”ëª¨ë¦¬ ìµœì í™”"""
        if df is None or df.empty:
            return df
        
        original_memory = df.memory_usage(deep=True).sum()
        optimized_df = df.copy()
        
        # ìˆ«ì ì»¬ëŸ¼ ìµœì í™”
        for col in optimized_df.select_dtypes(include=['int64']).columns:
            col_min = optimized_df[col].min()
            col_max = optimized_df[col].max()
            
            if col_min >= 0:  # ì–‘ìˆ˜ë§Œ
                if col_max < 255:
                    optimized_df[col] = optimized_df[col].astype('uint8')
                elif col_max < 65535:
                    optimized_df[col] = optimized_df[col].astype('uint16')
                elif col_max < 4294967295:
                    optimized_df[col] = optimized_df[col].astype('uint32')
            else:  # ìŒìˆ˜ í¬í•¨
                if col_min > -128 and col_max < 127:
                    optimized_df[col] = optimized_df[col].astype('int8')
                elif col_min > -32768 and col_max < 32767:
                    optimized_df[col] = optimized_df[col].astype('int16')
                elif col_min > -2147483648 and col_max < 2147483647:
                    optimized_df[col] = optimized_df[col].astype('int32')
        
        # Float ì»¬ëŸ¼ ìµœì í™”
        for col in optimized_df.select_dtypes(include=['float64']).columns:
            optimized_df[col] = pd.to_numeric(optimized_df[col], downcast='float')
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ ìµœì í™”
        for col in optimized_df.select_dtypes(include=['object']).columns:
            num_unique_values = len(optimized_df[col].unique())
            num_total_values = len(optimized_df[col])
            
            # ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜í• ì§€ ê²°ì • (ìœ ë‹ˆí¬ ê°’ì´ 50% ë¯¸ë§Œì¸ ê²½ìš°)
            if num_unique_values / num_total_values < 0.5:
                optimized_df[col] = optimized_df[col].astype('category')
        
        optimized_memory = optimized_df.memory_usage(deep=True).sum()
        memory_reduction = (original_memory - optimized_memory) / original_memory * 100
        
        self.perf_logger.info(f"DataFrame optimized: {memory_reduction:.1f}% memory reduction")
        
        return optimized_df
    
    def batch_process_groups(self, group_data: Dict, batch_size: int = 100):
        """ê·¸ë£¹ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬"""
        groups = list(group_data.items())
        batches = [groups[i:i + batch_size] for i in range(0, len(groups), batch_size)]
        
        self.perf_logger.info(f"Processing {len(groups)} groups in {len(batches)} batches")
        
        return batches
    
    def lazy_load_template_variables(self, template: str):
        """í…œí”Œë¦¿ ë³€ìˆ˜ ì§€ì—° ë¡œë”©"""
        if not hasattr(self, '_template_cache'):
            self._template_cache = {}
        
        template_hash = hashlib.md5(template.encode()).hexdigest()
        
        if template_hash not in self._template_cache:
            import re
            variables = set(re.findall(r'\{(\w+)(?::[^}]+)?\}', template))
            self._template_cache[template_hash] = variables
        
        return self._template_cache[template_hash]
    
    def preload_common_data(self):
        """ê³µí†µ ë°ì´í„° ì‚¬ì „ ë¡œë”©"""
        if 'preloaded_data' not in st.session_state:
            st.session_state.preloaded_data = {
                'korean_names': self._load_korean_names(),
                'bank_accounts': self._load_bank_accounts(),
                'common_templates': self._load_common_templates()
            }
    
    def _load_korean_names(self):
        """í•œêµ­ ì´ë¦„ ë°ì´í„° ë¡œë”©"""
        return [
            "ê¹€ë¯¼ì¤€", "ì´ì„œí˜„", "ë°•ë„ìœ¤", "ìµœì„œì¤€", "ì •í•˜ìœ¤", "ê°•ì§€ìš°", "ìœ¤ì„œì—°", "ì„ì¤€í˜",
            "ì˜¤ì‹œìš°", "í•œì˜ˆì¤€", "ì†¡ì§€ì•„", "ì•ˆë„í˜„", "ì¥ì„œí˜„", "ì¡°ê±´ìš°", "ì‹ ì§€ë¯¼", "í™ì˜ˆì€"
        ]
    
    def _load_bank_accounts(self):
        """ì€í–‰ ê³„ì¢Œ ë°ì´í„° ë¡œë”©"""
        return [
            "êµ­ë¯¼ì€í–‰ 123-456-789012 (ì£¼)ì—¬í–‰ì²˜ëŸ¼",
            "ì‹ í•œì€í–‰ 234-567-890123 (ì£¼)ì—¬í–‰ì²˜ëŸ¼",
            "ìš°ë¦¬ì€í–‰ 345-678-901234 (ì£¼)ì—¬í–‰ì²˜ëŸ¼"
        ]
    
    def _load_common_templates(self):
        """ê³µí†µ í…œí”Œë¦¿ ë¡œë”©"""
        return {
            'simple': 'ê°„ë‹¨í•œ ì”ê¸ˆ ì•ˆë‚´ í…œí”Œë¦¿',
            'detailed': 'ìƒì„¸í•œ ì”ê¸ˆ ì•ˆë‚´ í…œí”Œë¦¿'
        }
    
    def clear_cache(self, pattern: str = None):
        """ìºì‹œ ì •ë¦¬"""
        cleared_files = 0
        
        for filename in os.listdir(self.cache_dir):
            if filename.endswith('.pickle'):
                if pattern is None or pattern in filename:
                    try:
                        os.remove(os.path.join(self.cache_dir, filename))
                        cleared_files += 1
                    except Exception as e:
                        self.perf_logger.error(f"Cache clear error for {filename}: {e}")
        
        self.perf_logger.info(f"Cleared {cleared_files} cache files")
        return cleared_files
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        cache_files = []
        total_size = 0
        
        for filename in os.listdir(self.cache_dir):
            if filename.endswith('.pickle'):
                filepath = os.path.join(self.cache_dir, filename)
                file_size = os.path.getsize(filepath)
                file_age = time.time() - os.path.getmtime(filepath)
                
                cache_files.append({
                    'filename': filename,
                    'size_mb': file_size / 1024 / 1024,
                    'age_hours': file_age / 3600
                })
                total_size += file_size
        
        return {
            'total_files': len(cache_files),
            'total_size_mb': total_size / 1024 / 1024,
            'files': cache_files
        }


class StreamlitPerformanceMonitor:
    """Streamlit ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.optimizer = PerformanceOptimizer()
        self.metrics = {}
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if 'performance_metrics' not in st.session_state:
            st.session_state.performance_metrics = {
                'page_loads': 0,
                'file_uploads': 0,
                'message_generations': 0,
                'errors': 0,
                'start_time': time.time()
            }
    
    def track_event(self, event_type: str, details: Dict[str, Any] = None):
        """ì´ë²¤íŠ¸ ì¶”ì """
        if 'performance_metrics' not in st.session_state:
            self.start_monitoring()
        
        st.session_state.performance_metrics[event_type] = \
            st.session_state.performance_metrics.get(event_type, 0) + 1
        
        self.optimizer.perf_logger.info(f"Event: {event_type}, Details: {details}")
    
    def show_performance_dashboard(self):
        """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ í‘œì‹œ"""
        st.markdown("## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        
        if 'performance_metrics' not in st.session_state:
            st.info("ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
            return
        
        metrics = st.session_state.performance_metrics
        uptime = time.time() - metrics.get('start_time', time.time())
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ì—…íƒ€ì„", f"{uptime/3600:.1f}ì‹œê°„")
        with col2:
            st.metric("í˜ì´ì§€ ë¡œë“œ", metrics.get('page_loads', 0))
        with col3:
            st.metric("íŒŒì¼ ì—…ë¡œë“œ", metrics.get('file_uploads', 0))
        with col4:
            st.metric("ë©”ì‹œì§€ ìƒì„±", metrics.get('message_generations', 0))
        
        # ìºì‹œ í†µê³„
        st.markdown("### ğŸ’¾ ìºì‹œ í†µê³„")
        cache_stats = self.optimizer.get_cache_stats()
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ìºì‹œ íŒŒì¼ ìˆ˜", cache_stats['total_files'])
        with col2:
            st.metric("ìºì‹œ í¬ê¸°", f"{cache_stats['total_size_mb']:.1f}MB")
        
        # ìºì‹œ ê´€ë¦¬
        if st.button("ğŸ—‘ï¸ ìºì‹œ ì •ë¦¬"):
            cleared = self.optimizer.clear_cache()
            st.success(f"âœ… {cleared}ê°œ ìºì‹œ íŒŒì¼ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()
        
        # ì„¸ë¶€ ìºì‹œ ì •ë³´
        if cache_stats['files']:
            with st.expander("ğŸ“‹ ìºì‹œ íŒŒì¼ ìƒì„¸"):
                for file_info in cache_stats['files']:
                    st.write(f"ğŸ“„ {file_info['filename']}: {file_info['size_mb']:.2f}MB, {file_info['age_hours']:.1f}ì‹œê°„ ì „")
    
    def optimize_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ìµœì í™”"""
        # ë¶ˆí•„ìš”í•œ í° ë°ì´í„° ì œê±°
        large_keys = []
        for key, value in st.session_state.items():
            if hasattr(value, '__sizeof__'):
                size = value.__sizeof__()
                if size > 1024 * 1024:  # 1MB ì´ìƒ
                    large_keys.append((key, size))
        
        if large_keys:
            self.optimizer.perf_logger.warning(f"Large session state objects: {large_keys}")
    
    def memory_usage_alert(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³ """
        import psutil
        
        try:
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 80:
                st.warning(f"âš ï¸ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_percent:.1f}%")
                
                if st.button("ğŸ”„ ë©”ëª¨ë¦¬ ìµœì í™”"):
                    self.optimize_session_state()
                    self.optimizer.clear_cache("temp_")
                    st.success("âœ… ë©”ëª¨ë¦¬ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
        except ImportError:
            # psutilì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
            pass


# ì „ì—­ ì„±ëŠ¥ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
_global_optimizer = None

def get_optimizer():
    """ì „ì—­ ì„±ëŠ¥ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = PerformanceOptimizer()
    return _global_optimizer

def optimize_large_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """ëŒ€ìš©ëŸ‰ DataFrame ìµœì í™”"""
    optimizer = get_optimizer()
    return optimizer.optimize_dataframe(df)

@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def cached_excel_read(file_path: str, **kwargs):
    """ìºì‹œëœ Excel ì½ê¸°"""
    return pd.read_excel(file_path, **kwargs)

@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
def cached_template_processing(template: str, sample_data: dict):
    """ìºì‹œëœ í…œí”Œë¦¿ ì²˜ë¦¬"""
    try:
        return template.format(**sample_data)
    except Exception as e:
        return f"í…œí”Œë¦¿ ì˜¤ë¥˜: {str(e)}"

def performance_monitor(func_name: str = None):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    optimizer = get_optimizer()
    return optimizer.measure_time(func_name)

def setup_performance_optimization():
    """ì„±ëŠ¥ ìµœì í™” ì´ˆê¸° ì„¤ì •"""
    # Streamlit ì„¤ì • ìµœì í™”
    st.set_page_config(
        page_title="ì—¬í–‰ ì”ê¸ˆ ë¬¸ì ìƒì„±ê¸°",
        page_icon="âœˆï¸",
        layout="wide",
        initial_sidebar_state="expanded",
        menu_items={
            'Get Help': None,
            'Report a bug': None,
            'About': "ì—¬í–‰ ì”ê¸ˆ ë¬¸ì ìƒì„±ê¸° v1.0"
        }
    )
    
    # ì„±ëŠ¥ CSS ì¶”ê°€
    st.markdown("""
    <style>
        /* ë¹ ë¥¸ ë Œë”ë§ì„ ìœ„í•œ CSS ìµœì í™” */
        .main .block-container {
            max-width: 1200px;
            padding-top: 1rem;
        }
        
        /* ì• ë‹ˆë©”ì´ì…˜ ìµœì í™” */
        * {
            transition: none !important;
            animation-duration: 0s !important;
        }
        
        /* ìŠ¤í¬ë¡¤ ìµœì í™” */
        .main {
            overflow-anchor: none;
        }
        
        /* í°íŠ¸ ìµœì í™” */
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;600;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Noto Sans KR', sans-serif;
            font-display: swap;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = StreamlitPerformanceMonitor()
    monitor.start_monitoring()
    monitor.track_event('page_load')
    
    return monitor